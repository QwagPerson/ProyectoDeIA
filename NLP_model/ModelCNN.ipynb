{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddd6fabc-7ef3-43fc-9eef-5ec280ed4412",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "771474e5-1dfb-479c-a564-848da86af319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score, classification_report\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import re\n",
    "import numpy as np\n",
    "import nlpaug.augmenter.word as naw # To synonym, see  to more info https://github.com/makcedward/nlpaug\n",
    "import nlpaug.augmenter.char as nac\n",
    "from mtranslate import translate\n",
    "import pickle\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import os\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2470dc6f-ffa0-4f35-8da7-ad945c0f1111",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a48f04c5-5624-4117-8a86-da0f82b2bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handmade\n",
    "hand_nawel_0 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/asistire_nawel.csv\", sep = ';')\n",
    "hand_nawel_1 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/no_asistire_nawel.csv\", sep = ';')\n",
    "hand_nawel_2 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/reagendar_nawel.csv\", sep = ';')\n",
    "hand_nawel_3 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/pedir_nawel.csv\", sep = ';')\n",
    "\n",
    "hand_tavo_0 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/asistire_tavo.csv\", sep = ';')\n",
    "hand_tavo_1 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/no_asistire_tavo.csv\", sep = ';')\n",
    "hand_tavo_2 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/reagendar_tavo.csv\", sep = ';')\n",
    "hand_tavo_3 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/pedir_tavo.csv\", sep = ';')\n",
    "\n",
    "hand_martin_0 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/asistire_martin.csv\", sep = ';', encoding = 'latin-1')\n",
    "hand_martin_1 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/no_asistire_martin.csv\", sep = ';')\n",
    "hand_martin_2 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/reagendar_martin.csv\", sep = ';')\n",
    "hand_martin_3 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/pedir_martin.csv\", sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ba72231-1871-4461-bc93-56f34c259837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class    345\n",
       "text     345\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = pd.concat([hand_nawel_0, hand_nawel_1, hand_nawel_2, hand_nawel_3,\n",
    "                 hand_tavo_0, hand_tavo_1, hand_tavo_2, hand_tavo_3,\n",
    "                 hand_martin_0, hand_martin_1, hand_martin_2, hand_martin_3])\n",
    "\n",
    "df_.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbd33030-d54a-4705-997c-d6b7bec75008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "0    101\n",
       "1     89\n",
       "2     87\n",
       "3     68\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c310f0-c429-4bc1-9be9-4306c3770e3a",
   "metadata": {},
   "source": [
    "## Trabajando con la data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "712542cb-bf5b-4902-81a6-726ed44140b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.6.0/es_core_news_sm-3.6.0-py3-none-any.whl (12.9 MB)\n",
      "                                              0.0/12.9 MB ? eta -:--:--\n",
      "                                              0.1/12.9 MB 2.6 MB/s eta 0:00:05\n",
      "     ---                                      1.0/12.9 MB 13.0 MB/s eta 0:00:01\n",
      "     ----                                     1.3/12.9 MB 10.5 MB/s eta 0:00:02\n",
      "     --------                                 2.6/12.9 MB 15.3 MB/s eta 0:00:01\n",
      "     ---------                                3.1/12.9 MB 16.4 MB/s eta 0:00:01\n",
      "     -------------                            4.5/12.9 MB 16.9 MB/s eta 0:00:01\n",
      "     ---------------------                    6.9/12.9 MB 22.2 MB/s eta 0:00:01\n",
      "     -----------------------                  7.5/12.9 MB 20.9 MB/s eta 0:00:01\n",
      "     -----------------------------            9.4/12.9 MB 23.2 MB/s eta 0:00:01\n",
      "     ----------------------------------      11.3/12.9 MB 32.8 MB/s eta 0:00:01\n",
      "     -------------------------------------   12.5/12.9 MB 31.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.9/12.9 MB 29.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.9/12.9 MB 29.7 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.9/12.9 MB 24.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in c:\\users\\nahue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from es-core-news-sm==3.6.0) (3.6.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\nahue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\nahue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\nahue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\nahue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\nahue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\nahue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\nahue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\nahue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\nahue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\nahue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\nahue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\nahue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\nahue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\nahue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\nahue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\nahue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.10.9)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nahue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nahue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nahue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\nahue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\nahue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.6.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nahue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nahue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nahue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nahue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\nahue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\nahue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nahue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\nahue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nahue\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.1.2)\n",
      "\u001b[38;5;3m[!] As of spaCy v3.0, shortcuts like 'es' are deprecated. Please use\n",
      "the full pipeline package name 'es_core_news_sm' instead.\u001b[0m\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "# To vocabulary\n",
    "!python -m spacy download es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7cb4a053-ee99-40db-93af-443521347fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cb1ab752-ff81-46ed-9923-daab4a48394c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      " (2, ' veamos este tema otro dia por favor')\n",
      "Example:\n",
      " (0, 'voy a llegar a la hora ')\n",
      "Example:\n",
      " (0, 'lo tengo agendado y confirmado')\n",
      "Example:\n",
      " (1, 'se me complicó')\n",
      "Example:\n",
      " (2, 'me re-agenda la cita que teniamos a cualquier hora el sabado')\n"
     ]
    }
   ],
   "source": [
    "# Creation Of tuples (class, text)\n",
    "dataset = [\n",
    "    (row['class'], row['text'].lower()) # With lower\n",
    "    for index, row in df_.iterrows()\n",
    "]\n",
    "\n",
    "from random import sample\n",
    "for example in sample(dataset, 5):\n",
    "    print(\"Example:\\n\", example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e91bfb28-c710-407f-baa3-35bf517e2433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pum ahora hagamos la arquitectura\n",
    "# simplecita, un capa de embedding, y luego una red feed forward de\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Red neuronal con una sola capa oculta\n",
    "class ArgumentClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class, cnn_pool_channels = 24, cnn_kernel_size = 3):#, hidden_size, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        # capa de embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, mode=\"mean\")\n",
    "\n",
    "        # capas de la MLP\n",
    "        #self.fc = nn.Linear(embed_dim, num_class)\n",
    "        # self.fc1 = nn.Linear(embed_dim, hidden_size)\n",
    "        # self.fc2 = nn.Linear(hidden_size, num_class)\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=1,\n",
    "            out_channels=cnn_pool_channels,\n",
    "            kernel_size=cnn_kernel_size * embed_dim,\n",
    "            stride=embed_dim,\n",
    "        )\n",
    "\n",
    "        # Calculamos el tamaño de entrada de la capa lineal\n",
    "        fc_in_size = cnn_pool_channels\n",
    "\n",
    "        # Creamos la capa lineal\n",
    "        self.fc = nn.Linear(fc_in_size, num_classes)\n",
    "\n",
    "        # Inicializamos los pesos de las capas\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Definimos el rango de los valores iniciales de los pesos\n",
    "        initrange = 0.5\n",
    "\n",
    "        # Inicializamos los pesos de la capa de embedding\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "        # Inicializamos los pesos de la capa lineal\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "        # Inicializamos los sesgos de la capa lineal en cero\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, batch):\n",
    "         # Preparamos el input de la capa de embeddings a partir de text y offsets\n",
    "        padded_inputs, _, _ = batch\n",
    "        padded_inputs = torch.tensor(padded_inputs) \n",
    "        h = self.embedding(padded_inputs)\n",
    "        h = h.view(h.size(0), 1, -1)\n",
    "        h = torch.relu(self.conv(h))\n",
    "        h = h.mean(dim=2)\n",
    "        output = self.fc(h)\n",
    "        return F.log_softmax(output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a1c691ea-fe84-45f1-9e34-08b3853491bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary len: 436\n",
      "Random Words: ['atiendan', 'asistiré', 'wenaa', '<pad>', '27']\n",
      "Label's number: 4\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer(\"spacy\", \"es\")\n",
    "min_freq = 1\n",
    "vocab = build_vocab_from_iterator((tokenizer(text[1]) for text in train_split), min_freq=min_freq)\n",
    "labels = list({doc[0] for doc in train_split})\n",
    "label_map = {label: index for index, label in enumerate(labels)}\n",
    "\n",
    "UNK_IDX = 0\n",
    "vocab.set_default_index(UNK_IDX)\n",
    "\n",
    "vocab.insert_token('<pad>', 1)\n",
    "\n",
    "stoi = vocab.get_stoi()\n",
    "\n",
    "print(\"Vocabulary len:\", len(vocab))\n",
    "print(\"Random Words:\", sample(vocab.get_itos(), 5))\n",
    "print(\"Label's number:\", len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "52e19294-2307-4bab-a69f-f6a4f6a2e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have the required parameters for the model\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 32\n",
    "num_classes = 4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create instances of the custom Dataset class\n",
    "train_dataset = CustomDataset(train_data, tokenizer)\n",
    "test_dataset = CustomDataset(test_data, tokenizer)\n",
    "\n",
    "# Define data loaders with collate_fn\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model = ArgumentClassifier(vocab_size, embed_dim, num_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9a4dd19e-fced-46bc-811c-5e2f6e427a0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      3\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels, _ \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m      5\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m         labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[64], line 31\u001b[0m, in \u001b[0;36mcollate_fn\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     28\u001b[0m lengths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Pad the sequences to the maximum length in the batch\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m padded_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Convert the labels to a tensor\u001b[39;00m\n\u001b[0;32m     34\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(labels)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\utils\\rnn.py:399\u001b[0m, in \u001b[0;36mpad_sequence\u001b[1;34m(sequences, batch_first, padding_value)\u001b[0m\n\u001b[0;32m    395\u001b[0m         sequences \u001b[38;5;241m=\u001b[39m sequences\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    397\u001b[0m \u001b[38;5;66;03m# assuming trailing dimensions and type of all the Tensors\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;66;03m# in sequences are same and fetching those from sequences[0]\u001b[39;00m\n\u001b[1;32m--> 399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_value\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got list"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels, _ in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, _ in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            test_loss += criterion(outputs, labels).item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Test Loss: {test_loss/len(test_loader):.4f}, Accuracy: {(100 * correct / total):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29287b16-1457-48bc-890a-98c07ab41e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
