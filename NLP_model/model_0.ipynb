{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RrDk4gjfu6HL"
   },
   "source": [
    "# Proeycto de IA - ChatBox\n",
    "\n",
    "Departamento de Ciencias de la Computación, Universidad de Chile.\n",
    "\n",
    "CC6409: Taller de Desarrollo de Poryectod e IA - Otoño 2023\n",
    "\n",
    "**Integrantes:**\n",
    "- Garrido Martín\n",
    "- Gómez Nahuel\n",
    "- Santelices Gustavo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SugtK4Qlu19U"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XJbIlDWjqXHA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nahue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score, classification_report\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import re\n",
    "import numpy as np\n",
    "import nlpaug.augmenter.word as naw # To synonym, see  to more info https://github.com/makcedward/nlpaug\n",
    "import nlpaug.augmenter.char as nac\n",
    "from mtranslate import translate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47_Qf8Fiu3y7"
   },
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "J0NrlV_7ppMu"
   },
   "outputs": [],
   "source": [
    "# handmade\n",
    "hand_nawel_0 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/asistire_nawel.csv\", sep = ';')\n",
    "hand_nawel_1 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/no_asistire_nawel.csv\", sep = ';')\n",
    "hand_nawel_2 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/reagendar_nawel.csv\", sep = ';')\n",
    "hand_nawel_3 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/pedir_nawel.csv\", sep = ';')\n",
    "\n",
    "hand_tavo_0 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/asistire_tavo.csv\", sep = ';')\n",
    "hand_tavo_1 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/no_asistire_tavo.csv\", sep = ';')\n",
    "hand_tavo_2 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/reagendar_tavo.csv\", sep = ';')\n",
    "hand_tavo_3 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/pedir_tavo.csv\", sep = ';')\n",
    "\n",
    "hand_martin_0 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/asistire_martin.csv\", sep = ';', encoding = 'latin-1')\n",
    "hand_martin_1 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/no_asistire_martin.csv\", sep = ';')\n",
    "hand_martin_2 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/reagendar_martin.csv\", sep = ';')\n",
    "hand_martin_3 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/pedir_martin.csv\", sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "piVtRE1KtSBs",
    "outputId": "f4b39ef7-ea47-4f52-c511-2994cad69235"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class    345\n",
       "text     345\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = pd.concat([hand_nawel_0, hand_nawel_1, hand_nawel_2, hand_nawel_3,\n",
    "                 hand_tavo_0, hand_tavo_1, hand_tavo_2, hand_tavo_3,\n",
    "                 hand_martin_0, hand_martin_1, hand_martin_2, hand_martin_3])\n",
    "\n",
    "df_.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hbkIOlhkuNP8",
    "outputId": "8234e091-add4-4988-fb00-a280e82d1d8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "0    101\n",
       "1     89\n",
       "2     87\n",
       "3     68\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hbkIOlhkuNP8",
    "outputId": "8234e091-add4-4988-fb00-a280e82d1d8a"
   },
   "source": [
    "### Nahuel approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imported CHATGPT csv (the input to chatgpt are the created handmade)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sRwvvqnJqfKQ"
   },
   "outputs": [],
   "source": [
    "# GPT\n",
    "gpt_nawel_0 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/GPT_generated/asistire_nawel.csv\", sep = ';')\n",
    "gpt_nawel_1 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/GPT_generated/no_asistire_nawel.csv\", sep = ';')\n",
    "gpt_nawel_2 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/GPT_generated/reagendar_nawel.csv\", sep = ';')\n",
    "gpt_nawel_3 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/GPT_generated/pedir_nawel.csv\", sep = ';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con Bert\n",
    "aug = naw.ContextualWordEmbsAug(model_path='bert-base-multilingual-uncased', aug_p=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordnet - spa\n",
    "aug1 = naw.SynonymAug(aug_src='wordnet', lang='spa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asistir\n",
      "Por supuesto\n",
      "Bert: ['por ۔']\n",
      "Wordnet: ['Por suposición'] \n",
      "\n",
      "No Asistir\n",
      "No me siento preparado para rendir la prueba, cancelame la hora\n",
      "Bert: ['no me siento preparado para rendir la fiesta, durante la hora']\n",
      "Wordnet: ['No me siento preparado para rendir la conato, cancelame la hora'] \n",
      "\n",
      "Reagendar\n",
      "Puedes ayudarme a cambiar la hora para la licencia\n",
      "Bert: ['puedes llegar a cambiar la hora para la calle']\n",
      "Wordnet: ['Puedes ayudarme a tecla de mayúsculas la hora para la autorización'] \n",
      "\n",
      "Pedir\n",
      "Hola, ¿acá puedo pedir la hora para la licencia?\n",
      "Bert: ['asi, ¿ aca puedo pedir la hora para hacer licencia?']\n",
      "Wordnet: ['Qué tal, ¿ acá puedo pedir la cita para la licencia?'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Replace words with synonyms\n",
    "\n",
    "# Example for values in each sentences\n",
    "print('Asistir')\n",
    "print(hand_nawel_0.text[10])\n",
    "print(f'Bert: {aug.augment(hand_nawel_0.text[10])}') # show secoond row\n",
    "print(f'Wordnet: {aug1.augment(hand_nawel_0.text[10])} \\n')\n",
    "\n",
    "print('No Asistir')\n",
    "print(hand_nawel_1.text[10])\n",
    "print(f'Bert: {aug.augment(hand_nawel_1.text[10])}') # show secoond row\n",
    "print(f'Wordnet: {aug1.augment(hand_nawel_1.text[10])} \\n')\n",
    "\n",
    "print('Reagendar')\n",
    "print(hand_nawel_2.text[10])\n",
    "print(f'Bert: {aug.augment(hand_nawel_2.text[10])}') # show secoond row\n",
    "print(f'Wordnet: {aug1.augment(hand_nawel_2.text[10])} \\n')\n",
    "\n",
    "print('Pedir')\n",
    "print(hand_nawel_3.text[10])\n",
    "print(f'Bert: {aug.augment(hand_nawel_3.text[10])}') # show secoond row\n",
    "print(f'Wordnet: {aug1.augment(hand_nawel_3.text[10])} \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aug_key = nac.KeyboardAug() #Keyboard Augmenter: Substitute character by keyboard distance\n",
    "aug_ins = nac.RandomCharAug(action=\"insert\") # insert randomw characters\n",
    "aug_subs = nac.RandomCharAug(action=\"substitute\") # Substitute character randomly\n",
    "aug_swp = nac.RandomCharAug(action=\"swap\") # swap randomly\n",
    "aug_del = nac.RandomCharAug(action=\"delete\") # delete randomly\n",
    "#aug_mis = naw.SpellingAug() # Substitute word by spelling mistake words dictionary (inly english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asistir\n",
      "Por supuesto\n",
      "Keyboard: ['Por suOuWsHo']\n",
      "Random Insert: ['Por sIupnueesto']\n",
      "Random Substitution: ['Por suauesTA']\n",
      "Random Swap: ['Por uspeusot']\n",
      "Random delete: ['Por speso'] \n",
      "\n",
      "No Asistir\n",
      "No me siento preparado para rendir la prueba, cancelame la hora\n",
      "Keyboard: ['No me siento prrparXxo OZra rendir la prusbW, cancelame la ho$X']\n",
      "Random Insert: ['No me sUient1o preparado para renzdi7r la prue%b)a, cancelame la (hoOra']\n",
      "Random Substitution: ['No me siento preparado nary reddir la prueba, AanFelnme la porY']\n",
      "Random Swap: ['No me seinot preparaod praa rendir la prueba, acneclmae la hora']\n",
      "Random delete: ['No me seto preparado para rndi la prua, cancelame la hr']  \n",
      "\n",
      "Reagendar\n",
      "Puedes ayudarme a cambiar la hora para la licencia\n",
      "Keyboard: ['Puedes ayHdxFme a cajnLar la hora Oarq la licencia']\n",
      "Random Insert: ['Puedes ayudarme a cXambeiaEr la hora pzaOra la Xloi8cencia']\n",
      "Random Substitution: ['xuedWs ayudarme a Laeb_ar la hora para la l&2Sncia']\n",
      "Random Swap: ['Uepdes ayudarme a cambiar la hora para la licnecia']\n",
      "Random delete: ['Puds ayudarme a cmia la or para la licencia']  \n",
      "\n",
      "Pedir\n",
      "Hola, ¿acá puedo pedir la hora para la licencia?\n",
      "Keyboard: ['UolZ, ¿ acá 9u2do pedir la hora lqra la lis#bcia?']\n",
      "Random Insert: ['H7oqla, ¿ acá puQedGo pedir la yhorqa pwa3ra la licencia?']\n",
      "Random Substitution: ['Ho5I, ¿ acá puedo ze7ir la @jra para la liqe5ci#?']\n",
      "Random Swap: ['Hola, ¿ acá upeod pderi la ohar para la liceicna?']\n",
      "Random delete: ['Hola, ¿ acá pdo pei la ha pa la licencia?']  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. add noise\n",
    "\n",
    "# Example for values in each sentences\n",
    "\n",
    "print('Asistir')\n",
    "print(hand_nawel_0.text[10])\n",
    "print(f'Keyboard: {aug_key.augment(hand_nawel_0.text[10])}') # show secoond row\n",
    "print(f'Random Insert: {aug_ins.augment(hand_nawel_0.text[10])}') # show secoond row\n",
    "print(f'Random Substitution: {aug_subs.augment(hand_nawel_0.text[10])}') # show secoond row\n",
    "print(f'Random Swap: {aug_swp.augment(hand_nawel_0.text[10])}') # show secoond row\n",
    "print(f'Random delete: {aug_del.augment(hand_nawel_0.text[10])} \\n') # show secoond row\n",
    "\n",
    "print('No Asistir')\n",
    "print(hand_nawel_1.text[10])\n",
    "print(f'Keyboard: {aug_key.augment(hand_nawel_1.text[10])}') # show secoond row\n",
    "print(f'Random Insert: {aug_ins.augment(hand_nawel_1.text[10])}') # show secoond row\n",
    "print(f'Random Substitution: {aug_subs.augment(hand_nawel_1.text[10])}') # show secoond row\n",
    "print(f'Random Swap: {aug_swp.augment(hand_nawel_1.text[10])}') # show secoond row\n",
    "print(f'Random delete: {aug_del.augment(hand_nawel_1.text[10])}  \\n') # show secoond row\n",
    "\n",
    "print('Reagendar')\n",
    "print(hand_nawel_2.text[10])\n",
    "print(f'Keyboard: {aug_key.augment(hand_nawel_2.text[10])}') # show secoond row\n",
    "print(f'Random Insert: {aug_ins.augment(hand_nawel_2.text[10])}') # show secoond row\n",
    "print(f'Random Substitution: {aug_subs.augment(hand_nawel_2.text[10])}') # show secoond row\n",
    "print(f'Random Swap: {aug_swp.augment(hand_nawel_2.text[10])}') # show secoond row\n",
    "print(f'Random delete: {aug_del.augment(hand_nawel_2.text[10])}  \\n') # show secoond row\n",
    "\n",
    "print('Pedir')\n",
    "print(hand_nawel_3.text[10])\n",
    "print(f'Keyboard: {aug_key.augment(hand_nawel_3.text[10])}') # show secoond row\n",
    "print(f'Random Insert: {aug_ins.augment(hand_nawel_3.text[10])}') # show secoond row\n",
    "print(f'Random Substitution: {aug_subs.augment(hand_nawel_3.text[10])}') # show secoond row\n",
    "print(f'Random Swap: {aug_swp.augment(hand_nawel_3.text[10])}') # show secoond row\n",
    "print(f'Random delete: {aug_del.augment(hand_nawel_3.text[10])}  \\n') # show secoond row\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra : Translate Ideas\n",
    "def sp_to_en(data):\n",
    "    augmented_data = []\n",
    "    for sentence in data:\n",
    "        # Translate Spanish sentence to English\n",
    "        translated_sentence = translate(sentence, \"en\")\n",
    "        augmented_data.append(translated_sentence)\n",
    "\n",
    "    return augmented_data\n",
    "\n",
    "def en_to_sp(data):\n",
    "    augmented_data = []\n",
    "    for sentence in data:\n",
    "        # Translate English sentence to Spanish\n",
    "        translated_sentence = translate(sentence, \"es\")\n",
    "\n",
    "        augmented_data.append(translated_sentence)\n",
    "\n",
    "    return augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig: Hola, confirmo mi asistencia\n",
      "Eng: Hello, I confirm my attendance\n",
      "Spa: Hola confirmo mi asistencia \n",
      "\n",
      "Orig: si, confirmo\n",
      "Eng: yes, I confirm\n",
      "Spa: si, lo confirmo \n",
      "\n",
      "Orig: Si, asistiré a la fecha estipulada\n",
      "Eng: Yes, I will attend the stipulated date\n",
      "Spa: Sí, asistiré en la fecha estipulada. \n",
      "\n",
      "Orig: Si\n",
      "Eng: And\n",
      "Spa: Y \n",
      "\n",
      "Orig: Ok\n",
      "Eng: Ok\n",
      "Spa: De acuerdo \n",
      "\n",
      "Orig: okey\n",
      "Eng: okey\n",
      "Spa: bueno \n",
      "\n",
      "Orig: Por supuesto\n",
      "Eng: Of course\n",
      "Spa: Por supuesto \n",
      "\n",
      "Orig: Si, si puedo ir\n",
      "Eng: Yes, yes I can go\n",
      "Spa: si, si puedo ir \n",
      "\n",
      "Orig: Iré\n",
      "Eng: I will go\n",
      "Spa: voy a ir \n",
      "\n",
      "Orig: Claro, ahí estaré\n",
      "Eng: Sure, I'll be there\n",
      "Spa: Claro, estaré allí \n",
      "\n",
      "Orig: Por supuesto\n",
      "Eng: Of course\n",
      "Spa: Por supuesto \n",
      "\n",
      "Orig: SI, asistiré el martes\n",
      "Eng: YES, I will attend on Tuesday\n",
      "Spa: SI, atiendo el martes \n",
      "\n",
      "Orig: Hola, confirmo\n",
      "Eng: Hello, I confirm\n",
      "Spa: hola te confirmo \n",
      "\n",
      "Orig: CONFIRMO\n",
      "Eng: I CONFIRM\n",
      "Spa: CONFIRMO \n",
      "\n",
      "Orig: #voy\n",
      "Eng: #voy\n",
      "Spa: #voy \n",
      "\n",
      "Orig: Sin falta, voy\n",
      "Eng: Without fail, I go\n",
      "Spa: sin falta voy \n",
      "\n",
      "Orig: Obvio que si, me costó mucho tomarla\n",
      "Eng: Obviously yes, I had a hard time taking it\n",
      "Spa: Obvio que si, me costó tomarlo \n",
      "\n",
      "Orig: Claro que iré, aunque esté la cagá\n",
      "Eng: Of course I'll go, even if it's shit\n",
      "Spa: Por supuesto que iré, aunque sea una mierda. \n",
      "\n",
      "Orig: Por supuesto, ahí estaré\n",
      "Eng: Of course I'll be there\n",
      "Spa: por supuesto que estaré allí \n",
      "\n",
      "Orig: Hola, te hablo para confirmar la hora\n",
      "Eng: Hi, I'm calling to confirm the time\n",
      "Spa: hola llamo para confirmar hora \n",
      "\n",
      "Orig: Hola, te quieroa confirmar la hora de mañana\n",
      "Eng: Hi, I want to confirm tomorrow's time\n",
      "Spa: Hola quiero confirmar la hora de mañana \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This idea can be used to find synonyms in english.\n",
    "original = hand_nawel_0.text.tolist()\n",
    "augmented_sentences = sp_to_en(original)\n",
    "augmented_sentences2 = en_to_sp(augmented_sentences)\n",
    "\n",
    "for i in range(len(augmented_sentences2)):\n",
    "    print(f'Orig: {original[i]}')\n",
    "    print(f'Eng: {augmented_sentences[i]}')\n",
    "    print(f'Spa: {augmented_sentences2[i]} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a2GPoPe1fpG"
   },
   "source": [
    "## Custom Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "gxQn-gB91jds"
   },
   "outputs": [],
   "source": [
    "class PreProccesingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def preprocess(self,sentence):\n",
    "      # Deleting all except: exclamation/question signs and accents\n",
    "      new_word = re.sub(r\"[^a-zA-ZáéíóúÁÉÍÓÚñÑ¡!¿?\\s]\", '', sentence)\n",
    "      # Deleting double blank spaces\n",
    "      new_sentence = new_word.replace('  ',' ').replace('\\n','').strip()\n",
    "      return new_sentence\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        values = []\n",
    "        for tweet in X:\n",
    "            values.append(self.preprocess(tweet))\n",
    "\n",
    "        return(np.array(values))\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LJM2N-Uvx0j"
   },
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dewh_gkEyZNh"
   },
   "source": [
    "### Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "81FPMnTLM0Ts"
   },
   "outputs": [],
   "source": [
    "# Defining threshold\n",
    "thresholds = {\n",
    "    0: 0.85,  # class 0\n",
    "    1: 0.85,  # class 1\n",
    "    2: 0.85,   # class 2\n",
    "    3: 0.85   #class 3\n",
    "}\n",
    "\n",
    "\n",
    "def custom_predict(model, X, class_thresholds):\n",
    "    # Get probabilities\n",
    "    probabilities = model.predict_proba(X)\n",
    "\n",
    "    # Aplied threshold\n",
    "    modified_predictions = np.argmax(probabilities, axis=1)\n",
    "    for class_label, threshold in class_thresholds.items():\n",
    "        modified_predictions[probabilities[:, class_label] > threshold] = class_label\n",
    "\n",
    "    return modified_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GGE3LfHRysWI"
   },
   "outputs": [],
   "source": [
    "def run(dataset, pipeline):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        dataset['text'],\n",
    "        dataset['class'],\n",
    "        shuffle=True,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=dataset['class']\n",
    "    )\n",
    "\n",
    "    print(f\"# Len Training Data: {len(X_train)}\")\n",
    "    print(f\"# Len Testing Data: {len(X_test)}\")\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    # Classes propbailities\n",
    "    probabilities = pipeline.predict_proba(X_test)\n",
    "\n",
    "\n",
    "    # Predicted labels\n",
    "    #predicted_labels = pipeline.predict(X_test)\n",
    "    predicted_labels = custom_predict(pipeline, X_test, thresholds)\n",
    "\n",
    "\n",
    "    print(classification_report(y_test, predicted_labels))\n",
    "\n",
    "\n",
    "    return pipeline, X_train, X_test, y_train, y_test, predicted_labels, probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIMBgRmCyd6Y"
   },
   "source": [
    "## Defining Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RoiPLpB6wNgx"
   },
   "outputs": [],
   "source": [
    "# simple Pipeline only BOW\n",
    "\n",
    "def get_experiment_0_pipeline():\n",
    "\n",
    "    return Pipeline(\n",
    "        [\n",
    "            (\n",
    "                \"features\",\n",
    "                FeatureUnion(\n",
    "                    [\n",
    "                        (\"bow\", CountVectorizer()),\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            (\"clf\", MultinomialNB()),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tqZbmwfz2UTM"
   },
   "outputs": [],
   "source": [
    "# simple Pipeline only Preprocessing + BOW\n",
    "\n",
    "def get_experiment_1_pipeline():\n",
    "\n",
    "    return Pipeline(\n",
    "        [\n",
    "            (\"preprocessing\", PreProccesingTransformer()),\n",
    "            (\n",
    "                \"features\",\n",
    "                FeatureUnion(\n",
    "                    [\n",
    "                        (\"bow\", CountVectorizer())\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            (\"clf\", MultinomialNB()),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7cZ4DHxxQMJ"
   },
   "source": [
    "## Implementación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p2Jy6hTjyG5A",
    "outputId": "a9d1f332-560b-40fc-ad95-95d1ee4b157a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Len Training Data: 286\n",
      "# Len Testing Data: 72\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.79      0.79        24\n",
      "           1       0.76      0.70      0.73        23\n",
      "           2       0.75      0.43      0.55         7\n",
      "           3       0.78      1.00      0.88        18\n",
      "\n",
      "    accuracy                           0.78        72\n",
      "   macro avg       0.77      0.73      0.74        72\n",
      "weighted avg       0.78      0.78      0.77        72\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple Pipeline Test\n",
    "pipeline0 = get_experiment_0_pipeline()\n",
    "\n",
    "prob = run(df_, pipeline0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bnQh_0k42teE",
    "outputId": "00072766-027a-459f-8702-38a52b11b0af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Len Training Data: 286\n",
      "# Len Testing Data: 72\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.79      0.79        24\n",
      "           1       0.76      0.70      0.73        23\n",
      "           2       0.75      0.43      0.55         7\n",
      "           3       0.78      1.00      0.88        18\n",
      "\n",
      "    accuracy                           0.78        72\n",
      "   macro avg       0.77      0.73      0.74        72\n",
      "weighted avg       0.78      0.78      0.77        72\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple Pipeline Test (+ preprocessing)\n",
    "pipeline1 = get_experiment_1_pipeline()\n",
    "\n",
    "_ = run(df_, pipeline1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
