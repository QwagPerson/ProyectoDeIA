{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RrDk4gjfu6HL"
   },
   "source": [
    "# Proeycto de IA - ChatBox\n",
    "\n",
    "Departamento de Ciencias de la Computación, Universidad de Chile.\n",
    "\n",
    "CC6409: Taller de Desarrollo de Poryectod e IA - Otoño 2023\n",
    "\n",
    "**Integrantes:**\n",
    "- Garrido Martín\n",
    "- Gómez Nahuel\n",
    "- Santelices Gustavo\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SugtK4Qlu19U"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XJbIlDWjqXHA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nahue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score, classification_report\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import re\n",
    "import numpy as np\n",
    "import nlpaug.augmenter.word as naw # To synonym, see  to more info https://github.com/makcedward/nlpaug\n",
    "import nlpaug.augmenter.char as nac\n",
    "from mtranslate import translate\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "47_Qf8Fiu3y7"
   },
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "J0NrlV_7ppMu"
   },
   "outputs": [],
   "source": [
    "# handmade\n",
    "hand_nawel_0 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/asistire_nawel.csv\", sep = ';')\n",
    "hand_nawel_1 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/no_asistire_nawel.csv\", sep = ';')\n",
    "hand_nawel_2 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/reagendar_nawel.csv\", sep = ';')\n",
    "hand_nawel_3 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/pedir_nawel.csv\", sep = ';')\n",
    "\n",
    "hand_tavo_0 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/asistire_tavo.csv\", sep = ';')\n",
    "hand_tavo_1 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/no_asistire_tavo.csv\", sep = ';')\n",
    "hand_tavo_2 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/reagendar_tavo.csv\", sep = ';')\n",
    "hand_tavo_3 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/pedir_tavo.csv\", sep = ';')\n",
    "\n",
    "hand_martin_0 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/asistire_martin.csv\", sep = ';', encoding = 'latin-1')\n",
    "hand_martin_1 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/no_asistire_martin.csv\", sep = ';')\n",
    "hand_martin_2 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/reagendar_martin.csv\", sep = ';')\n",
    "hand_martin_3 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/Handmade/pedir_martin.csv\", sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "piVtRE1KtSBs",
    "outputId": "f4b39ef7-ea47-4f52-c511-2994cad69235"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class    345\n",
       "text     345\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = pd.concat([hand_nawel_0, hand_nawel_1, hand_nawel_2, hand_nawel_3,\n",
    "                 hand_tavo_0, hand_tavo_1, hand_tavo_2, hand_tavo_3,\n",
    "                 hand_martin_0, hand_martin_1, hand_martin_2, hand_martin_3])\n",
    "\n",
    "df_.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hbkIOlhkuNP8",
    "outputId": "8234e091-add4-4988-fb00-a280e82d1d8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "0    101\n",
       "1     89\n",
       "2     87\n",
       "3     68\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_['class'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation (in a first step, to all parameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ChatGPT sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nahuel\n",
    "gpt_nawel_0 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/GPT_generated/asistire_nawel.csv\", sep = ';')\n",
    "gpt_nawel_1 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/GPT_generated/no_asistire_nawel.csv\", sep = ';')\n",
    "gpt_nawel_2 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/GPT_generated/reagendar_nawel.csv\", sep = ';')\n",
    "gpt_nawel_3 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/GPT_generated/pedir_nawel.csv\", sep = ';')\n",
    "\n",
    "# Martin\n",
    "gpt_martin_0 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/GPT_generated/asistire_martin.csv\", sep = ';')\n",
    "gpt_martin_1 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/GPT_generated/no_asistire_martin.csv\", sep = ';')\n",
    "gpt_martin_2 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/GPT_generated/reagendar_martin.csv\", sep = ';')\n",
    "gpt_martin_3 = pd.read_csv(\"https://raw.githubusercontent.com/QwagPerson/ProyectoDeIA/main/Data/GPT_generated/pedir_martin.csv\", sep = ';')\n",
    "\n",
    "# Gustavo\n",
    "#gpt_tavo_0 = pd.read_csv(\"\", sep = ';')\n",
    "#gpt_tavo_1 = pd.read_csv(\"\", sep = ';')\n",
    "#gpt_tavo_2 = pd.read_csv(\"\", sep = ';')\n",
    "#gpt_tavo_3 = pd.read_csv(\"\", sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat the chatgpt sentences\n",
    "df_ = pd.concat([df_, \n",
    "                 gpt_nawel_0, gpt_nawel_1, gpt_nawel_2, gpt_nawel_3,\n",
    "                 gpt_martin_0, gpt_martin_1, gpt_martin_2, gpt_martin_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "0    185\n",
       "1    178\n",
       "2    162\n",
       "3    141\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_['class'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate augmentation (functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra : Translate Ideas\n",
    "def sp_to_en(data):\n",
    "    augmented_data = []\n",
    "    for sentence in data:\n",
    "        # Translate Spanish sentence to English\n",
    "        translated_sentence = translate(sentence, \"en\")\n",
    "        augmented_data.append(translated_sentence)\n",
    "\n",
    "    return augmented_data\n",
    "\n",
    "def en_to_sp(data):\n",
    "    augmented_data = []\n",
    "    for sentence in data:\n",
    "        # Translate English sentence to Spanish\n",
    "        translated_sentence = translate(sentence, \"es\")\n",
    "\n",
    "        augmented_data.append(translated_sentence)\n",
    "\n",
    "    return augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model\n",
    "aug_fb = naw.BackTranslationAug(\n",
    "    from_model_name='facebook/wmt19-en-de', \n",
    "    to_model_name='facebook/wmt19-de-en'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Translate_aug(df_):\n",
    "    # Getting the values with two or more words.\n",
    "    df_english_aug = df_[df_['text'].apply(lambda x: len(x.split()) > 2)]\n",
    "\n",
    "    # Traslate to English\n",
    "    l_english_aug = sp_to_en(df_english_aug.text.tolist())\n",
    "\n",
    "    # Find Synonyms\n",
    "    english_syn_aug = aug_fb.augment(l_english_aug)\n",
    "\n",
    "    # Translate to spanish\n",
    "    l_english_aug = en_to_sp(df_english_aug.text.tolist())\n",
    "\n",
    "    # Append text with label\n",
    "    df_english = pd.DataFrame()\n",
    "    df_english['class'] = df_english_aug['class']\n",
    "    df_english['text'] = l_english_aug\n",
    "\n",
    "    return df_english"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data aug in spanish (functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordnet - spa\n",
    "aug_wn = naw.SynonymAug(aug_src='wordnet', lang='spa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spanish_aug(df_):\n",
    "    # Getting the values with ten or more words.\n",
    "    df_spanish_aug = df_[df_['text'].apply(lambda x: len(x.split()) > 10)]\n",
    "    l_augmented = aug_wn.augment(df_spanish_aug.text.tolist())\n",
    "\n",
    "    # Append text with label\n",
    "    df_spanish = pd.DataFrame()\n",
    "    df_spanish['class'] = df_spanish_aug['class']\n",
    "    df_spanish['text'] = l_augmented\n",
    "\n",
    "    return df_spanish"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "0a2GPoPe1fpG"
   },
   "source": [
    "## Custom Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "gxQn-gB91jds"
   },
   "outputs": [],
   "source": [
    "class PreProccesingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def preprocess(self,sentence):\n",
    "      # Deleting all except: exclamation/question signs and accents\n",
    "      new_word = re.sub(r\"[^a-zA-ZáéíóúÁÉÍÓÚñÑ¡!¿?\\s]\", '', sentence)\n",
    "      # Deleting double blank spaces\n",
    "      new_sentence = new_word.replace('  ',' ').replace('\\n','').strip()\n",
    "      return new_sentence\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        values = []\n",
    "        for tweet in X:\n",
    "            values.append(self.preprocess(tweet))\n",
    "\n",
    "        return(np.array(values))\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "6LJM2N-Uvx0j"
   },
   "source": [
    "## Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dewh_gkEyZNh"
   },
   "source": [
    "### Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "81FPMnTLM0Ts"
   },
   "outputs": [],
   "source": [
    "# Defining threshold\n",
    "thresholds = {\n",
    "    0: 0.90,  # class 0\n",
    "    1: 0.80,  # class 1\n",
    "    2: 0.80,   # class 2\n",
    "    3: 0.80   #class 3\n",
    "}\n",
    "\n",
    "\n",
    "def custom_predict(model, X, class_thresholds):\n",
    "    # Get probabilities\n",
    "    probabilities = model.predict_proba(X)\n",
    "\n",
    "    # Aplied threshold\n",
    "    modified_predictions = np.argmax(probabilities, axis=1)\n",
    "    for class_label, threshold in class_thresholds.items():\n",
    "        modified_predictions[probabilities[:, class_label] > threshold] = class_label\n",
    "\n",
    "    return modified_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "GGE3LfHRysWI"
   },
   "outputs": [],
   "source": [
    "def run(dataset, pipeline):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        dataset['text'],\n",
    "        dataset['class'],\n",
    "        shuffle=True,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=dataset['class']\n",
    "    )\n",
    "\n",
    "    print(f\"# Len Training Data: {len(X_train)}\")\n",
    "    print(f\"# Len Testing Data: {len(X_test)}\")\n",
    "\n",
    "    print('Aplying data aug')\n",
    "\n",
    "    df_to_aug = pd.DataFrame({'text': X_train, 'class': y_train})\n",
    "\n",
    "    english_df = Translate_aug(df_to_aug)\n",
    "    spanish_df = spanish_aug(df_to_aug)\n",
    "\n",
    "    X_train = pd.concat([X_train, english_df['text'], spanish_df['text']])\n",
    "    y_train = pd.concat([y_train, english_df['class'], spanish_df['class']])\n",
    "\n",
    "    print(f\"# NEW Len Training Data: {len(X_train)}\")\n",
    "    print(f\"# NEW Len Testing Data: {len(X_test)}\")\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    # Classes propbailities\n",
    "    probabilities = pipeline.predict_proba(X_test)\n",
    "\n",
    "\n",
    "    # Predicted labels\n",
    "    predicted_labels = pipeline.predict(X_test)\n",
    "    #predicted_labels = custom_predict(pipeline, X_test, thresholds)\n",
    "\n",
    "\n",
    "    print(classification_report(y_test, predicted_labels))\n",
    "\n",
    "\n",
    "    return pipeline, X_train, X_test, y_train, y_test, predicted_labels, probabilities\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wIMBgRmCyd6Y"
   },
   "source": [
    "## Defining Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "RoiPLpB6wNgx"
   },
   "outputs": [],
   "source": [
    "# simple Pipeline only BOW\n",
    "\n",
    "def get_experiment_0_pipeline():\n",
    "\n",
    "    return Pipeline(\n",
    "        [\n",
    "            (\n",
    "                \"features\",\n",
    "                FeatureUnion(\n",
    "                    [\n",
    "                        (\"bow\", CountVectorizer()),\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            (\"clf\", MultinomialNB()),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "tqZbmwfz2UTM"
   },
   "outputs": [],
   "source": [
    "# simple Pipeline only Preprocessing + BOW\n",
    "\n",
    "def get_experiment_1_pipeline():\n",
    "\n",
    "    return Pipeline(\n",
    "        [\n",
    "            (\"preprocessing\", PreProccesingTransformer()),\n",
    "            (\n",
    "                \"features\",\n",
    "                FeatureUnion(\n",
    "                    [\n",
    "                        (\"bow\", CountVectorizer())\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            (\"clf\", MultinomialNB()),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "f7cZ4DHxxQMJ"
   },
   "source": [
    "## Implementación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p2Jy6hTjyG5A",
    "outputId": "a9d1f332-560b-40fc-ad95-95d1ee4b157a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Len Training Data: 532\n",
      "# Len Testing Data: 134\n",
      "Aplying data aug\n",
      "# NEW Len Training Data: 1181\n",
      "# NEW Len Testing Data: 134\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.78      0.85        37\n",
      "           1       0.84      0.86      0.85        36\n",
      "           2       0.79      0.94      0.86        33\n",
      "           3       0.93      0.89      0.91        28\n",
      "\n",
      "    accuracy                           0.87       134\n",
      "   macro avg       0.87      0.87      0.87       134\n",
      "weighted avg       0.87      0.87      0.87       134\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple Pipeline Test\n",
    "pipeline0 = get_experiment_0_pipeline()\n",
    "\n",
    "prob = run(df_, pipeline0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(pipeline0.predict(['Cancela mi hora']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "with open('model0.bin', 'wb') as fil:\n",
    "    pickle.dump(pipeline0, fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sup_file import PreProccesingTransformer\n",
    "\n",
    "with open(r\"C:\\Users\\nahue\\OneDrive\\Escritorio\\Universidad\\9no_Semestre\\CC6409_Proyecto de IA\\ProyectoDeIA\\NLP_model\\model0.bin\", 'rb') as archivo:\n",
    "    modelo = pickle.load(archivo)\n",
    "\n",
    "print(modelo.predict(['Cancela mi hora']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bnQh_0k42teE",
    "outputId": "00072766-027a-459f-8702-38a52b11b0af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Len Training Data: 532\n",
      "# Len Testing Data: 134\n",
      "Aplying data aug\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Simple Pipeline Test (+ preprocessing)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m pipeline1 \u001b[39m=\u001b[39m get_experiment_1_pipeline()\n\u001b[1;32m----> 4\u001b[0m _ \u001b[39m=\u001b[39m run(df_, pipeline1)\n",
      "Cell \u001b[1;32mIn[17], line 19\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(dataset, pipeline)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mAplying data aug\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m df_to_aug \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m: X_train, \u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m: y_train})\n\u001b[1;32m---> 19\u001b[0m english_df \u001b[39m=\u001b[39m Translate_aug(df_to_aug)\n\u001b[0;32m     20\u001b[0m spanish_df \u001b[39m=\u001b[39m spanish_aug(df_to_aug)\n\u001b[0;32m     22\u001b[0m X_train \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([X_train, english_df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m], spanish_df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]])\n",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m, in \u001b[0;36mTranslate_aug\u001b[1;34m(df_)\u001b[0m\n\u001b[0;32m      3\u001b[0m df_english_aug \u001b[39m=\u001b[39m df_[df_[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39mlen\u001b[39m(x\u001b[39m.\u001b[39msplit()) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m)]\n\u001b[0;32m      5\u001b[0m \u001b[39m# Traslate to English\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m l_english_aug \u001b[39m=\u001b[39m sp_to_en(df_english_aug\u001b[39m.\u001b[39;49mtext\u001b[39m.\u001b[39;49mtolist())\n\u001b[0;32m      8\u001b[0m \u001b[39m# Find Synonyms\u001b[39;00m\n\u001b[0;32m      9\u001b[0m english_syn_aug \u001b[39m=\u001b[39m aug_fb\u001b[39m.\u001b[39maugment(l_english_aug)\n",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m, in \u001b[0;36msp_to_en\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      3\u001b[0m augmented_data \u001b[39m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m data:\n\u001b[0;32m      5\u001b[0m     \u001b[39m# Translate Spanish sentence to English\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     translated_sentence \u001b[39m=\u001b[39m translate(sentence, \u001b[39m\"\u001b[39;49m\u001b[39men\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      7\u001b[0m     augmented_data\u001b[39m.\u001b[39mappend(translated_sentence)\n\u001b[0;32m      9\u001b[0m \u001b[39mreturn\u001b[39;00m augmented_data\n",
      "File \u001b[1;32mc:\\Users\\nahue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mtranslate\\core.py:80\u001b[0m, in \u001b[0;36mtranslate\u001b[1;34m(to_translate, to_language, from_language)\u001b[0m\n\u001b[0;32m     78\u001b[0m     link \u001b[39m=\u001b[39m base_link \u001b[39m%\u001b[39m (to_language, from_language, to_translate)\n\u001b[0;32m     79\u001b[0m     request \u001b[39m=\u001b[39m urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39mRequest(link, headers\u001b[39m=\u001b[39magent)\n\u001b[1;32m---> 80\u001b[0m     raw_data \u001b[39m=\u001b[39m urllib\u001b[39m.\u001b[39;49mrequest\u001b[39m.\u001b[39;49murlopen(request)\u001b[39m.\u001b[39mread()\n\u001b[0;32m     81\u001b[0m data \u001b[39m=\u001b[39m raw_data\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m expr \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(?s)class=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(?:t0|result-container)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m>(.*?)<\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\nahue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[1;32m--> 216\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n",
      "File \u001b[1;32mc:\\Users\\nahue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    516\u001b[0m     req \u001b[39m=\u001b[39m meth(req)\n\u001b[0;32m    518\u001b[0m sys\u001b[39m.\u001b[39maudit(\u001b[39m'\u001b[39m\u001b[39murllib.Request\u001b[39m\u001b[39m'\u001b[39m, req\u001b[39m.\u001b[39mfull_url, req\u001b[39m.\u001b[39mdata, req\u001b[39m.\u001b[39mheaders, req\u001b[39m.\u001b[39mget_method())\n\u001b[1;32m--> 519\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(req, data)\n\u001b[0;32m    521\u001b[0m \u001b[39m# post-process response\u001b[39;00m\n\u001b[0;32m    522\u001b[0m meth_name \u001b[39m=\u001b[39m protocol\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_response\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\nahue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m    535\u001b[0m protocol \u001b[39m=\u001b[39m req\u001b[39m.\u001b[39mtype\n\u001b[1;32m--> 536\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_open, protocol, protocol \u001b[39m+\u001b[39;49m\n\u001b[0;32m    537\u001b[0m                           \u001b[39m'\u001b[39;49m\u001b[39m_open\u001b[39;49m\u001b[39m'\u001b[39;49m, req)\n\u001b[0;32m    538\u001b[0m \u001b[39mif\u001b[39;00m result:\n\u001b[0;32m    539\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\nahue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    497\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\nahue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:1377\u001b[0m, in \u001b[0;36mHTTPHandler.http_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttp_open\u001b[39m(\u001b[39mself\u001b[39m, req):\n\u001b[1;32m-> 1377\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_open(http\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mHTTPConnection, req)\n",
      "File \u001b[1;32mc:\\Users\\nahue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\urllib\\request.py:1348\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1347\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1348\u001b[0m         h\u001b[39m.\u001b[39;49mrequest(req\u001b[39m.\u001b[39;49mget_method(), req\u001b[39m.\u001b[39;49mselector, req\u001b[39m.\u001b[39;49mdata, headers,\n\u001b[0;32m   1349\u001b[0m                   encode_chunked\u001b[39m=\u001b[39;49mreq\u001b[39m.\u001b[39;49mhas_header(\u001b[39m'\u001b[39;49m\u001b[39mTransfer-encoding\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m   1350\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n\u001b[0;32m   1351\u001b[0m         \u001b[39mraise\u001b[39;00m URLError(err)\n",
      "File \u001b[1;32mc:\\Users\\nahue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:1283\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1280\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\u001b[39mself\u001b[39m, method, url, body\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, headers\u001b[39m=\u001b[39m{}, \u001b[39m*\u001b[39m,\n\u001b[0;32m   1281\u001b[0m             encode_chunked\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m   1282\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1283\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[1;32mc:\\Users\\nahue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:1329\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(body, \u001b[39mstr\u001b[39m):\n\u001b[0;32m   1326\u001b[0m     \u001b[39m# RFC 2616 Section 3.7.1 says that text default has a\u001b[39;00m\n\u001b[0;32m   1327\u001b[0m     \u001b[39m# default charset of iso-8859-1.\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m     body \u001b[39m=\u001b[39m _encode(body, \u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m-> 1329\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders(body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[1;32mc:\\Users\\nahue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:1278\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1276\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1277\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1278\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[1;32mc:\\Users\\nahue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:1038\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1036\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer)\n\u001b[0;32m   1037\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1038\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(msg)\n\u001b[0;32m   1040\u001b[0m \u001b[39mif\u001b[39;00m message_body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1041\u001b[0m \n\u001b[0;32m   1042\u001b[0m     \u001b[39m# create a consistent interface to message_body\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(message_body, \u001b[39m'\u001b[39m\u001b[39mread\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m   1044\u001b[0m         \u001b[39m# Let file-like take precedence over byte-like.  This\u001b[39;00m\n\u001b[0;32m   1045\u001b[0m         \u001b[39m# is needed to allow the current position of mmap'ed\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m         \u001b[39m# files to be taken into account.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nahue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:976\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    974\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    975\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_open:\n\u001b[1;32m--> 976\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m    977\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    978\u001b[0m         \u001b[39mraise\u001b[39;00m NotConnected()\n",
      "File \u001b[1;32mc:\\Users\\nahue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:942\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Connect to the host and port specified in __init__.\"\"\"\u001b[39;00m\n\u001b[0;32m    941\u001b[0m sys\u001b[39m.\u001b[39maudit(\u001b[39m\"\u001b[39m\u001b[39mhttp.client.connect\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mport)\n\u001b[1;32m--> 942\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_connection(\n\u001b[0;32m    943\u001b[0m     (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhost,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource_address)\n\u001b[0;32m    944\u001b[0m \u001b[39m# Might fail in OSs that don't implement TCP_NODELAY\u001b[39;00m\n\u001b[0;32m    945\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\nahue\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:833\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    831\u001b[0m \u001b[39mif\u001b[39;00m source_address:\n\u001b[0;32m    832\u001b[0m     sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[1;32m--> 833\u001b[0m sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[0;32m    834\u001b[0m \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[0;32m    835\u001b[0m err \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Simple Pipeline Test (+ preprocessing)\n",
    "pipeline1 = get_experiment_1_pipeline()\n",
    "\n",
    "_ = run(df_, pipeline1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "with open('model1.bin', 'wb') as fil:\n",
    "    pickle.dump(pipeline1, fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Respuesta al usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_label= {\n",
    "    0: 'Asistir a la',\n",
    "    1: 'Cancelar la',\n",
    "    2: 'Reagendar la',\n",
    "    3: 'Pedir una'\n",
    "}\n",
    "\n",
    "options = {\n",
    "    0: 'Su reserva a quedado confirmada para el DD de MM',\n",
    "    1: 'Su reserva con fecha DD de MM a quedado cancelada',\n",
    "    2: 'Las fechas para reagendar son las siguientes',\n",
    "    3: 'Las fechas próximas son las siguientes'\n",
    "}\n",
    "\n",
    "def manage_mesages(target):\n",
    "    # Make a prediction\n",
    "    label = pipeline1.predict([target])[0]\n",
    "    print(f\"usted quiere {map_label[label]} hora\")\n",
    "    print(options[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usted quiere Cancelar la hora\n",
      "Su reserva con fecha DD de MM a quedado cancelada\n"
     ]
    }
   ],
   "source": [
    "manage_mesages('Puedo cancelar la hora?')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
